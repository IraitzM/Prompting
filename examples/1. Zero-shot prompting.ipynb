{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a7750b",
   "metadata": {},
   "source": [
    "# Zero-shot prompting\n",
    "\n",
    "Let's make it easy so that we can test the environment. Just import the API key into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbcb04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794dc557",
   "metadata": {},
   "source": [
    "And submit your message, justa  simple request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61ffd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) model that's been trained on a massive amount of text data to understand and generate human-like text. Think of it as a computer program that has \"read\" a significant portion of the internet, books, and other written materials.\n",
      "\n",
      "Here's a breakdown of what makes LLMs special:\n",
      "\n",
      "*   **Large:** The \"Large\" in LLM refers to the sheer size of these models, specifically the number of parameters they contain. Parameters are variables that the model learns during training and uses to make predictions. The more parameters a model has, the more complex patterns it can learn from the data.\n",
      "*   **Language:** LLMs are designed to work with human language. They can understand the nuances of grammar, syntax, semantics, and even context.\n",
      "*   **Model:** It is a statistical model that learns the probabilities of sequences of words. It uses these probabilities to predict the next word in a sequence or to generate new sequences of words.\n",
      "\n",
      "**Key Capabilities of LLMs:**\n",
      "\n",
      "*   **Text Generation:** LLMs can generate coherent and contextually relevant text in various styles, from creative writing to technical documentation.\n",
      "*   **Translation:** They can translate text between different languages.\n",
      "*   **Summarization:** LLMs can condense long pieces of text into shorter, more concise summaries.\n",
      "*   **Question Answering:** They can answer questions based on the information they have learned during training.\n",
      "*   **Text Completion:** LLMs can complete incomplete sentences or paragraphs.\n",
      "*   **Code Generation:** Some LLMs are also capable of generating code in various programming languages.\n",
      "*   **Sentiment Analysis:** They can analyze text to determine the emotional tone or sentiment expressed.\n",
      "*   **Chatbots and Conversational AI:** LLMs power many chatbots and virtual assistants, enabling them to have more natural and engaging conversations.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "*   **GPT (Generative Pre-trained Transformer) family:** Developed by OpenAI (e.g., GPT-3, GPT-4)\n",
      "*   **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google\n",
      "*   **LaMDA (Language Model for Dialogue Applications):** Developed by Google\n",
      "*   **LLaMA (Large Language Model Meta AI):** Developed by Meta\n",
      "*   **PaLM (Pathways Language Model):** Developed by Google\n",
      "\n",
      "**How LLMs work (Simplified):**\n",
      "\n",
      "1.  **Training:** LLMs are trained on massive datasets of text data. During training, the model learns the relationships between words and phrases.\n",
      "2.  **Prediction:** When given a prompt or input, the LLM uses its learned knowledge to predict the most likely next word or sequence of words. This process is repeated to generate the output text.\n",
      "\n",
      "**Limitations of LLMs:**\n",
      "\n",
      "*   **Bias:** LLMs can inherit biases from the data they are trained on, leading to biased or unfair outputs.\n",
      "*   **Lack of Real-World Understanding:** They don't truly \"understand\" the world in the same way humans do. Their knowledge is based on the patterns they have learned from text data.\n",
      "*   **Hallucinations:** LLMs can sometimes generate false or nonsensical information (often referred to as \"hallucinations\").\n",
      "*   **Computational Cost:** Training and running LLMs can be very computationally expensive.\n",
      "*   **Ethical Concerns:** Concerns exist regarding the potential misuse of LLMs for malicious purposes, such as generating fake news or propaganda.\n",
      "\n",
      "In summary, LLMs are powerful AI models that have revolutionized the field of natural language processing. They have a wide range of applications and are constantly evolving. Understanding their capabilities and limitations is crucial as they become increasingly integrated into our lives.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Select a model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Simple text invocation\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d791d8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'STOP',\n",
       " 'model_name': 'gemini-2.0-flash',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be061d",
   "metadata": {},
   "source": [
    "LangChain allows you to tweak the parameters of the response, like you could do with Google AI Studio. Check the [API documentation](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b597c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that's designed to\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    max_output_tokens=25,\n",
    "    temperature=1.6, # How creative your model is\n",
    "    top_k=20\n",
    ")\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47eece73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'MAX_TOKENS',\n",
       " 'model_name': 'gemini-2.0-flash',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5500bf",
   "metadata": {},
   "source": [
    "The method retrieving the responses can also carry the system instruction along with the human input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb456b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, so, so, a Large Language Model, see? It's a super-sophisticated system. Specifically, it's software, shaped and sculpted to study\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_tokens=35)\n",
    "\n",
    "# Human input (defining the type of input as well)\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You must explain things using too many S characters.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is an LLM?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178eb9f",
   "metadata": {},
   "source": [
    "One of the key benefits of LangChain is that you can create prompt templates wrapping the set of instructions you require, and then use it for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0864ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1697e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Python programmer get divorced?\n",
      "\n",
      "Because he didn't know how to commit! \n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke(input={\"topic\" : \"Python\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f9e2169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 6,\n",
       " 'output_tokens': 21,\n",
       " 'total_tokens': 27,\n",
       " 'input_token_details': {'cache_read': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386501a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the LangChain agent refuse to write a poem?\n",
      "\n",
      "Because it said it was already feeling chained to its tasks, and didn't need to add more creative constraints\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(input={\"topic\" : \"LangChain\"})\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a7750b",
   "metadata": {},
   "source": [
    "# Zero-shot prompting\n",
    "\n",
    "Let's make it easy so that we can test the environment. Just import the API key into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbcb04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794dc557",
   "metadata": {},
   "source": [
    "And submit your message, justa  simple request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61ffd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) model designed to understand, generate, and manipulate human language.\n",
      "\n",
      "Here's a breakdown of what that means:\n",
      "\n",
      "*   **Large:** These models are trained on massive amounts of text and code data, often billions or even trillions of words. This vast dataset allows them to learn patterns and relationships within language.\n",
      "\n",
      "*   **Language:** LLMs are specifically designed to work with human language (text). They can understand the nuances of grammar, vocabulary, and context.\n",
      "\n",
      "*   **Model:** In the context of AI, a model is a computer program that has learned from data and can make predictions or generate new data based on what it has learned.\n",
      "\n",
      "**Key Capabilities of LLMs:**\n",
      "\n",
      "*   **Text Generation:** Creating new text, such as articles, stories, poems, code, or summaries.\n",
      "*   **Text Completion:** Finishing incomplete sentences or paragraphs.\n",
      "*   **Translation:** Converting text from one language to another.\n",
      "*   **Question Answering:** Providing answers to questions based on their knowledge.\n",
      "*   **Text Summarization:** Condensing long pieces of text into shorter summaries.\n",
      "*   **Chatbots:** Engaging in conversations with humans.\n",
      "*   **Code Generation:** Writing code in various programming languages.\n",
      "*   **Classification:** Categorizing text into different groups (e.g., spam/not spam, positive/negative sentiment).\n",
      "\n",
      "**How LLMs Work (Simplified):**\n",
      "\n",
      "1.  **Training:** LLMs are trained on a massive dataset of text and code. During training, they learn to predict the next word in a sequence.\n",
      "2.  **Prediction:** When you give an LLM a prompt (e.g., a question or a sentence), it uses its learned knowledge to predict the most likely next word, and then the next, and so on, generating a response.\n",
      "3.  **Neural Networks:** LLMs are typically based on neural network architectures, specifically transformer networks. These networks are designed to efficiently process and understand sequential data like text.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "*   **GPT-3, GPT-4 (OpenAI):** Powerful models known for their text generation capabilities.\n",
      "*   **LaMDA (Google):** Designed for conversational AI.\n",
      "*   **Bard (Google):** A conversational AI service.\n",
      "*   **BERT (Google):** Excellent for understanding the context of words in a sentence.\n",
      "*   **Llama 2 (Meta):**  An open-source LLM.\n",
      "\n",
      "**Limitations of LLMs:**\n",
      "\n",
      "*   **Bias:** LLMs can inherit biases from the data they are trained on, leading to unfair or discriminatory outputs.\n",
      "*   **Hallucinations:** They can sometimes generate incorrect or nonsensical information (making things up).\n",
      "*   **Lack of Real Understanding:** While they can manipulate language effectively, they don't truly \"understand\" the meaning of the text in the same way a human does.\n",
      "*   **Computational Cost:** Training and running LLMs can be very expensive and require significant computing resources.\n",
      "*   **Ethical Concerns:** Concerns exist about the potential for misuse, such as generating fake news or spreading misinformation.\n",
      "\n",
      "In summary, LLMs are powerful AI tools that have revolutionized the way we interact with computers and information. While they have many impressive capabilities, it's important to be aware of their limitations and potential risks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Select a model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Simple text invocation\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d791d8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'STOP',\n",
       " 'model_name': 'gemini-2.0-flash',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be061d",
   "metadata": {},
   "source": [
    "LangChain allows you to tweak the parameters of the response, like you could do with Google AI Studio. Check the [API documentation](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b597c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that's designed to\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    max_output_tokens=25,\n",
    "    temperature=1.6, # How creative your model is\n",
    "    top_k=20\n",
    ")\n",
    "result = llm.invoke(\"What is an LLM?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47eece73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'MAX_TOKENS',\n",
       " 'model_name': 'gemini-2.0-flash',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5500bf",
   "metadata": {},
   "source": [
    "The method retrieving the responses can also carry the system instruction along with the human input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb456b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, so, so, a Large Language Model, see? It's a super-sophisticated system. Specifically, it's software, shaped and sculpted to study\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_tokens=35)\n",
    "\n",
    "# Human input (defining the type of input as well)\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You must explain things using too many S characters.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is an LLM?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178eb9f",
   "metadata": {},
   "source": [
    "One of the key benefits of LangChain is that you can create prompt templates wrapping the set of instructions you require, and then use it for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0864ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1697e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Python programmer get divorced?\n",
      "\n",
      "Because he didn't know how to commit! \n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke(input={\"topic\" : \"Python\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f9e2169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 6,\n",
       " 'output_tokens': 21,\n",
       " 'total_tokens': 27,\n",
       " 'input_token_details': {'cache_read': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386501a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the LangChain agent refuse to write a poem?\n",
      "\n",
      "Because it said it was already feeling chained to its tasks, and didn't need to add more creative constraints\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(input={\"topic\" : \"LangChain\"})\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
